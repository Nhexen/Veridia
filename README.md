# ğŸ§  Veridia

**AI Code Assistant powered by your own LLM models**

Veridia is a modern, self-hosted AI client that connects to your local LLM models via LM Studio. It provides intelligent code analysis, generation, and development assistance while keeping your data completely private.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)
![React](https://img.shields.io/badge/react-18.2%2B-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-Latest-green.svg)

## âœ¨ Features

- ğŸ”’ **100% Private**: Your code never leaves your machine
- ğŸš€ **Modern UI**: Clean React interface with Material-UI
- âš¡ **Fast API**: Python FastAPI backend for optimal performance
- ğŸ¯ **Code-Focused**: Specialized for development workflows
- ğŸ”§ **Easy Setup**: Simple configuration with LM Studio
- ğŸ“± **Responsive**: Works on desktop and mobile browsers

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+**
- **Node.js 16+**
- **LM Studio** ([Download here](https://lmstudio.ai/))

### 1. Download LM Studio & Models

1. Download and install [LM Studio](https://lmstudio.ai/)
2. Download a recommended model (see [Recommended Models](#-recommended-models))
3. Start LM Studio and load your model
4. Enable the API server (default: `http://localhost:1234`)

### 2. Install Veridia

```bash
# Clone the repository
git clone https://github.com/nhexen/veridia.git
cd veridia

# Install backend dependencies
cd backend
pip install -r requirements.txt

# Install frontend dependencies
cd ../frontend
npm install
```

### 3. Configure & Run

```bash
# Start the backend (from project root)
cd backend
uvicorn main:app --reload

# Start the frontend (in another terminal)
cd frontend
npm start
```

ğŸ‰ **Veridia is now running!**
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000

## ğŸ¤– Recommended Models

### For Code Generation
| Model                 | Size   | Best For                           | Download                                                                        |
| --------------------- | ------ | ---------------------------------- | ------------------------------------------------------------------------------- |
| **DeepSeek Coder V2** | 16B    | General coding, multiple languages | [Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct) |
| **Code Llama**        | 7B-34B | Python, JavaScript, debugging      | [Meta AI](https://github.com/facebookresearch/codellama)                        |
| **Phind CodeLlama**   | 34B    | Complex problem solving            | [Hugging Face](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2)             |
| **WizardCoder**       | 15B    | Code explanation, refactoring      | [Hugging Face](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0)            |

### For General Development
| Model            | Size   | Best For             | Download                                                                    |
| ---------------- | ------ | -------------------- | --------------------------------------------------------------------------- |
| **Mixtral 8x7B** | 8x7B   | Balanced performance | [Hugging Face](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) |
| **Llama 2**      | 7B-70B | General purpose      | [Meta AI](https://github.com/facebookresearch/llama)                        |

## âš™ï¸ Configuration

### LM Studio Setup
1. Open LM Studio
2. Go to **Developer** tab
3. Start the server on `localhost:1234`
4. Note your model name

### Veridia Configuration
Update `backend/main.py` with your model details:

```python
LM_STUDIO_API_URL = "http://localhost:1234/v1/chat/completions"
# Update the model name in the generate_code function
"model": "your-model-name-here"
```

## ğŸ“ Project Structure

```
veridia/
â”œâ”€â”€ backend/              # FastAPI backend
â”‚   â”œâ”€â”€ main.py          # API endpoints
â”‚   â””â”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ frontend/            # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js       # Main component
â”‚   â”‚   â””â”€â”€ index.js     # Entry point
â”‚   â”œâ”€â”€ public/          # Static files
â”‚   â””â”€â”€ package.json     # Node dependencies
â”œâ”€â”€ .github/             # GitHub configuration
â””â”€â”€ README.md            # This file
```

## ğŸ› ï¸ Development

### Backend Development
```bash
cd backend
# Install in development mode
pip install -e .
# Run with auto-reload
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Frontend Development
```bash
cd frontend
# Start development server
npm start
# Build for production
npm run build
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **[LM Studio](https://lmstudio.ai/)** - For the excellent local LLM runtime
- **[FastAPI](https://fastapi.tiangolo.com/)** - For the modern Python web framework
- **[React](https://reactjs.org/)** - For the powerful frontend library
- **[FontAwesome](https://fontawesome.com/)** - For the beautiful component library

## ğŸ“ Support

- ğŸ› [Report Issues](https://github.com/nhexen/veridia/issues)
- ğŸ’¬ [Discussions](https://github.com/nhexen/veridia/discussions)
- ğŸ“§ [Email Support](mailto:support@veridia.dev)

---

**Made for developers who value privacy and control over their AI tools.**
